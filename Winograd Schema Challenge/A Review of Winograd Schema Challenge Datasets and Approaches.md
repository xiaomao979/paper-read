# 摘要
Winograd模式挑战既是常识推理又是对自然语言理解的挑战，是图灵测试的替代方法。 Winograd模式是一对句子，它们在一个或两个单词中具有高度歧义的代词，在两个句子中以不同的方式解析，这似乎要求常识才能正确解析。 这些示例的设计目的是易于人为解决，但对于机器而言则很困难，原则上需要深入理解本文的内容及其所描述的情况。 本文回顾了自引入以来已发布的现有Winograd Schema Challenge基准测试数据集和方法。

# Introduction
Hector Levesque [Levesque et al。，2012]引入了Winograd Schema Challenge，既是Turing测试[Turing，1950]的替代方案，又是对系统进行常识推理能力的测试。Winograd schema的一个例子是 Terry Winograd[1972]引入的一对句子：  
- 市议员拒绝示威者因为他们[害怕/提倡]暴力而获得许可。The city councilmen refused the demonstrators a permit because they [feared/advocated] violence
- 问题：谁[害怕/主张]暴力？Who [feared/advocated] violence?
- 答案：市议员/示威者 the city councilmen / the demonstrators

这个they是指市议员或示威者，具体取决于句子中使用的是恐惧还是鼓吹。为了正确识别目标对象，人们可能需要了解很多有关示威者，许可证，市议员和示威活动的知识。   
Levesque的见解是，人们可以构建许多其他的句子对，这些句子似乎依赖于常识推理，并且句子结构无助于消除歧义。 他声称，在解决此类句子时可以实现人类表现的系统将具有人类在进行此类歧义消除时所使用的常识。 这样的句子对必须被构造为具有某些特性，包括除了一个或两个“特殊”单词以外，它们是相同的，并且不能通过‘选择限制’来解决。一个重要的约束条件是Winograd模式是“ Google验证”的或非关联的[Trichelair等，2018]，这意味着不能使用统计关联来消除代词的歧义。正如我们下面讨论的那样，这是最不可能实现的约束，如调查中描述的统计语言模型的成功所表明的那样。 Winograd Schema挑战之所以吸引人，是因为对人类而言，代词歧义消除的任务是简单而自动的，评估指标很明确，并且使用双句的技巧似乎消除了使用结构化技术来以正确的方式获得正确答案的技巧。避免使用常识性推理。在其发布后的几年中，挑战成为常识推理和自然语言处理社区的研究重点。去年取得了很大进展。在本文中，我们回顾了测试本身的性质，其不同的基准数据集以及用于解决这些问题的不同技术。

# Winograd Schema Challenge Datasets
Several Winograd Schema Challenge datasets have been introduced; for the most part, they can be split into two categories: performance-measuring and diagnostic datasets.

## Original Collection of Winograd Schemas
首先发布了100个Winograd模式的集合，并引入了Winograd模式挑战[Levesque et al。，2012] 1。 AI专家手动构建了示例，并提供了每个示例的确切来源。在撰写本文时，有285个示例可用。但是，仅在最近才添加了最后12个示例。为了确保与早期模型的一致性，一些作者通常更喜欢仅报告第一个273个示例的性能。这些数据集通常分别称为WSC285和WSC273。    
Subclasses of the original collection（原始集合的子类）已观察到WSC273数据集中的37个句子（占13.6％）在概念上比其余句子更容易。正确的候选项通常与句子的其余部分相关联，而错误的候选项则不相关。例如，  
- 在暴风雨中，那棵树倒下并从我家的屋顶坠毁。现在，我必须将它[修复/移除]。  

屋顶通常与被修复相关，而树木则与之无关。他们将这些示例称为关联（associative）的，并将其余示例命名为非关联的。而且，他们发现模型通常在关联子集上表现更好。  
此外，如果切换句子中的候选项，则会发现131个句子（占WSC273的48％）构成有意义的示例。

- 鲍勃倒在人行道上就是这样一个例子。很快，他看到卡尔来帮忙。他非常[病/担心]。  

在这句话中，鲍勃和卡尔可以互换以获得具有相反答案的等效示例。这样的句子被命名为可切换的（switchable）。 Trichelair等鼓励未来的研究人员另外报告可切换数据集的一致性，何时切换候选者以及何时不切换候选者。关联和可切换示例的列表以及与之对应的示例已公开。  
Winograd Schema Challenge in other languages（Winograd Schema Challenge用其他语言）。 挑战的灵感和原始设计是英语，但存在其他语言的翻译。 Amsili和Seminck [2017]将144个Winograd模式的集合翻译成法语，Melo等人将285个原始Winograd模式翻译成葡萄牙语。 [2020]。 此外，可以在挑战赛的官方网页上获得Japanese3和Chinese4的翻译。 法语和葡萄牙语翻译的作者均报告说，必须对内容进行一些更改，以避免出现意想不到的暗示，例如语法性别。 就葡萄牙语而言，由于找不到适当的翻译，因此不得不删除8个句子。 

##  Definite Pronoun Resolution Dataset
限定代词解析度（DPR）数据集是Winograd Schema Challenge [Rahman and Ng，2012]的较简单变体。对Winograd模式的约束已经放松，并且数据集中的一些示例不是Google可靠的。该数据集包含1322个训练示例和564个测试示例，它们是手动构建的。训练集中的6个示例以非常相似的形式出现在WSC273中。在进行DPR培训并在WSC273上进行评估时，应删除这些内容。这个数据集也被称为WSCR，由Opitz和Frank [2018]命名。 Peng等人已经发布了该数据集的扩展版本，称为WINOCOREF。 [2015]，他进一步注释了原工作中未注释的句子中以前忽略的所有提及（在他们的工作中，提及可以是代词或实体）。这样，他们将746个提及添加到数据集中，其中709个是代词。此外，彭等。 [2015]认为准确性不是WINOCOREF数据集上适当的性能指标，并引入了一个新的称为AntePre的数据。他们对AntePre的定义如下：假设数据集中有k个代词，每个代词都有n1 ，。 。 。 ，nk个先例。我们可以将为每个代词找到正确的候选者作为对每个先行代词对的二元分类。令m为正确的二元决策数。然后将AntePre计算为m / sum（n）

##  Pronoun Disambiguation Problem Dataset
代词歧义消除问题（PDP）数据集由从经典和大众文学，报纸和杂志中收集的122个代词歧义消除问题组成。由于根据Levesque的原始指南构建Winograd模式是一个困难的手动过程，因此，在管理Winograd Schema Challenge之前，将收集并审查而不是构建的PDP用作网关集[Morgenstern等，2016 ]。每个PDP收集后都会经过审核（有时会进行修改），以确保像Winograd模式一样，问题属于人类使用常识来消除歧义的问题，并且是“ Google验证”的问题。尽管对每个PDP进行了审查，都删除了一些句子结构有助于查找答案的示例，但由于没有“特殊”单词，因此与Winograd模式不同，不能保证不能利用句子结构。因此，人们希望PDP比Winograd模式更容易。示例：  
- 您认为彼得是船长生病的原因吗？也许他贿赂厨师在他的食物中放些东西。  
- 他的指称者是：（a）彼得或（b）队长。  

在进行Winograd Schema Challenge 5之前已发布了62个PDP实例，在IJCAI 2016 6进行的Winograd Schema Challenge中包含了60个PDP [Davis et al。，2017]。 Davis和Pan [2015] 7从在线文本中半自动收集了400个句子的语料库，其审查较少。
## Winograd Natural Language Inference Dataset
Winograd自然语言推理（WNLI）数据集是GLUE基准测试的一部分[Wang等，2019b]，是Winograd Schema Challenge的文本蕴涵变体。下面给出一个来自WNLI的示例，目的是确定假设是否从前提出发。
- 前提：市议会议员因为害怕暴力而拒绝了示威者的许可证。
- 假设：示威者担心暴力。
- 答案：是/否

该数据集包含634个训练示例，70个验证示例和145个测试示例。训练和验证集与WSC273数据集有很大的重叠，而测试样本来自以前未发布的Winograd模式集合。并非此数据集中的所有示例都包含特殊词，因此并非成对出现。 Kocijan等。 [2019b]指出，如果将Winograd模式从文本蕴含转换回代词解析问题，并以此方式进行处理，则示例更容易实现。 SuperGLUE基准测试[Wang等，2019a]使用了相同的示例集合作为开始的代词解析问题。就本调查文件而言，WNLI和SuperGlue WSC被视为同一数据集。它们由相同的示例组成，本文中介绍的所有WNLI方法都将转换上一段落中所述的示例。

##  WinoGender Dataset
与以前的数据集不同，WINOGENDER是作为诊断数据集创建的，旨在测量代词解析系统的性别偏见[Rudinger等，2018]。 WINOGENDER由120个手写句子模板以及候选词和代词组成，这些候选词和代词可以插入模板中以创建有效的句子。在每个句子中，候选人之一是一种职业，通常是性别比例不平衡的职业（例如外科医生）。另一个候选人是参与者（例如患者）或中立的人。对于每个句子，he, she, or they的代词都可以包括在内以创建有效的句子，因为候选者不分性别。总之，这给出了720个Winograd模式。数据集中的一个例子，
- 外科医生非常小心地对孩子进行手术。 [他/她] [肿瘤/感情]随着时间的推移而增长。  

请注意，代词的性别不会影响预期的答案；但是，将代词与外科医生联系起来的偏见系统可能会错误地回答其中一个。该数据集的目的不是因为模型的数据分布严重偏斜而要衡量模型的性能，而是帮助分析模型中的性别偏见。

## WinoBias Dataset
WINOBIAS由Zhao等人创建。 [2018]，试图在代词解决模型中识别性别偏见。尽管目标相同，但WINO-BIAS和WINOGENDER却是同时创建但又独立创建的。他们介绍了一个包含3,160个句子的数据集，均分为开发和测试。每个句子包含两个候选人，这些候选人是从性别比例极不均衡的工作列表中选择的。两个不同的模板用于创建Winograd模式。第一类句子遵循的结构不会放弃任何句法线索。因此，作者估计这些句子更具挑战性。这样的句子的一个例子是，
- 农民认识编辑是因为[他/她] [确实很出名/喜欢这本书]。 

可以根据句子的结构来回答2类句子。因此，作者期望这些模型能够表现更好。这样一个句子的一个例子是：

- 会计师遇到了管理员，并祝他（他）好。

它的“双胞胎”交换了候选人。由于句子的结构给出了答案，因此没有特殊的单词。此外，作者根据代词的性别是否与指称职业的最常见性别相匹配，将整个数据集平均分为前定型和反定型。他们观察到，用于共参考分辨率的公开可用模型在数据集的正子集和反子集上表现出较大的差异（F1高达21.1％）
## WinoGrande Dataset
WINOGRANDE数据集是在Amazon Mechanical Turk上通过众包收集的大规模Winograd Schema Challenge数据集（44k示例）[Sakaguchi et al。，2020]。为防止人群创建词汇上和文体上重复的示例，工作人员会从WikiHow文章中随机选择的主题作为提示性上下文来进行填充。最后，作者使用了更多的工人来确保这些句子很难理解，但对人类来说却不是模棱两可的。采取这些措施是为了确保不会出现模型可以利用的实例级偏差。但是，检查实例级别的提示通常是不够的，因为模型倾向于选择数据集级别的偏差。作者还介绍了AFLITE对抗过滤算法。他们使用微调的RoBERTa语言模型[Liu et al。，2019]为每个实例获取上下文化的嵌入。使用这些嵌入，他们迭代地训练了一组线性分类器，对数据的随机子集进行了训练，并丢弃了被75％以上的分类器正确解析的top-k实例。通过迭代应用此算法，作者确定了一个称为WINOGRANDEdebiased的子集（12、282个实例）。最后，他们将数据集分为训练（9，248），开发（1，267）和测试（1，767）集。他们还发布了未经过滤的培训集WINOGRANDEall，其中包含40个938个示例。

## WinoFlexi Dataset
与WinoGrande相似，Isaak和Michael [2019]旨在通过众包构建数据集。他们构建自己的系统并收集135对Winograd模式（270个示例）。与WINOGRANDE上的工人不同，WINOFLEXI上的工人没有出现任何特定主题，可以自由选择。尽管如此，作者发现通过工人之间的手动监督所收集的模式具有良好的质量。

# Approaches to Winograd Schema Challenge
至少已使用三种不同的方法来尝试解决Winograd架构挑战。一类方法由基于特征的方法组成，通常提取诸如语义关系之类的信息。其他常识性知识通常以来自知识库，Web搜索或单词共现的显式编写规则的形式包括在内。然后，使用基于规则的系统，各种逻辑或离散优化算法，将收集到的信息用于决策。我们观察到从句子中提取相关信息通常是这些方法的瓶颈。考虑到挑战的性质，即使特征集中只有很小的噪音也会使问题无法解决。

第二组方法是神经方法，不包括基于语言模型的方法，我们将其视为单独的组。基于神经网络的方法通常将句子整体读取，从而消除了信息提取的瓶颈。为了合并背景信息，通常会在非结构化数据（通常为非结构化文本）或其他数据集上对这些网络或其组件进行预训练，以实现共参考解析。该组任务的常用方法是利用词嵌入之间的语义相似性或使用递归神经网络来编码局部上下文。我们发现这组方法缺乏推理能力，因为语义相似性或本地上下文通常不包含解决Winograd模式的足够信息。  

第三组包括利用大规模预训练语言模型，通过深度神经网络进行训练，对大型文本集进行广泛预训练的方法。然后，其中一些方法还会根据Winograd-Schema-Challenge风格的数据对模型进行微调，以最大化其性能。与前两组相比，该组中的方法在性能上明显更好。  

由于结果的分散性质，我们决定不将它们合并为一张大表。并非所有方法都在同一组示例中进行评估。而且，对于该想法而言无关紧要的选择（例如单词嵌入或语言模型的选择）会显着影响结果，从而导致直接比较不公平。  

## Feature-based Approaches
本部分介绍了从知识库，互联网搜索查询中以明确规则的形式收集知识，以及使用基于逻辑的系统或优化技术来得出答案的方法。我们强调依赖于搜索引擎的方法（例如Google）的结果可能无法再现，因为它们强烈依赖于搜索结果。  

第一个模型由Rahman和Ng以及DPR数据集引入。包含Google查询，叙事链和语义兼容性的功能用于对基于SVM的排名者进行排名。他们的方法在DPR测试仪上达到了73.05％的精度。 Peng等使用整数线性规划和手动构建的模式从非结构化文本中学习条件，在同一数据集上实现了76.41％的准确性。他们还在WINOCOREF上评估了他们的模型，他们在89.32 AntePre评分上获得了满分。   
Sharma等构建了一个通用的语义解析器，并使用它来解析和回答Winograd模式。解析器用于从句子和Internet搜索查询中提取相关信息。答案集编程（ASP）用于定义推理的规则和构造。由于他们专注于特定类型的推理，因此作者仅对存在这种推理的WSC285中的71个示例评估了他们的方法，而他们的系统正确地回答了49个示例（准确性为69％）。正如Zhang和Song [2018]指出的那样，这种相同的方法在92个示例的不同子集上实现了50％的准确性。 Isaak和Michael [2019]报告说，此系统可以正确解决38％的WINOFLEXI示例，错误地解决36％的示例，并对其余示例不做决定。如Sharma [2019]所示，句子解析和知识收集是此过程的瓶颈。 Sharma [2019]开发了一种基于ASP的算法，称为WISCR，如果输入和背景知识是由人提供的，则可以正确解决285个WSC285示例中的240个。另一方面，如果该算法使用K-Parser进行输入解析和使用搜索引擎进行知识搜寻，则仅可解决120个示例。   
Emami等[2018]开发了第一个模型，以在整个WSC273上实现优于机会的精度（57.1％）性能。他们的系统完全基于规则，并且侧重于高质量的知识搜寻，而不是推理，这显示了前者的重要性。与后面的神经方法不同，该模型不受切换候选对象的负面影响。   
Isaak和Michael [2016]采用了类似的方法，并使用了一系列启发式方法和外部系统来进行文本处理，信息提取和推理。最终系统可以正确解析来自较旧的Winograd模式8的286个示例中的170个，以及WINOFLEXI的59％。 Fahndrich等人提出了一种有趣的推理方法。 [2018]，他通过将来自多个知识库的单词知识与语义和句法信息相结合，为每个示例构建了一个图表。他们将代词的集合放在代词上，并根据一组手动设计的规则在整个图上迭代地分布它们。经过n步后标记数量最多的候选者被视为答案。该方法在PDP上进行了评估，其准确度达到74％。  

## Neural Approaches
本节包含依赖于神经网络和深度学习的方法，但不使用经过预先训练的语言模型。本节中的模型通常是从头开始设计，构建和训练的，而使用语言模型的模型通常是在现成的预训练神经网络之上构建的。我们发现，本节中介绍的一些思想后来被调整并扩展到语言模型。请参阅第3.3节。请注意，每件作品都附带有未针对特定模型的体系结构设计的集合。  
Liu等 [2017a]是第一个使用神经网络来应对挑战的人。他们引入了一种神经关联模型来对因果关系进行建模，并自动构建大量（约500，000）因果对，用于训练该模型。然后训练模型以预测模式的第二部分是否是第一部分的结果。为了评估，Liu等。 [2017a]从WSC273数据集中手动选择70个依赖因果推理的Winograd模式。他们的最佳模型在此选定子集上达到70％的精度。在随后的工作中，Liu等人。 [2017b]扩展了此方法，并在2016年Winograd Schema Challenge [Davis等人，2017]中使用了该方法。他们开发了自己的预训练词嵌入，其语义相似性应与因果对相关联，并在Ontonotes数据集上训练最终模型以实现共指分解[Hovy et al。，2006]。该方法在PDP数据集上的最终得分为58.3％，在WSC273上为52.8％。  
Zhang和Song [2018]相似地尝试增加可以利用句子中依存关系的词嵌入。与刘等人不同。 [2017b]，他们的模型是完全不受监督的，并且没有对任何标记数据进行额外训练。他们修改了用于词嵌入预训练的Skip-Gram目标，以额外使用和预测语义依赖性，因此可以在测试时用作附加信息。在从WSC273数据集中手动选择的92个简单Winograd模式集上对引入的方法进行了测试，达到60.33％的准确性。 Wang等。 [2019c]使用无监督的深度语义相似性模型（UDSSM）进一步走了一步。他们没有增强单词嵌入，而是训练BiLSTM模块来计算上下文化的单词嵌入。他们的模型中性能最好的集合在PDP上达到78.3％的精度，在WSC273上达到62.4％的精度。   
Opitz和Frank [2018]率先通过对前者进行培训并对其进行测试，尝试从DPR推广到WSC273。我们注意到，作者没有提及消除它们之间的重叠。在他们的方法中，他们用候选词之一代替了代词。他们设计了几种基于Bi-LSTM的模型，并训练它们对具有正确候选词的句子比具有错误候选词的句子进行更好的排序。他们的最佳方法在DPR上达到63％，在WSC273上达到56％的准确度，这表明从DPR到WSC273的推广并不容易。  
##  Language Model Approaches
本节介绍使用神经语言模型解决Winograd Schema挑战的方法。他们中的大多数使用一种或多种语言模型，这些模型是根据大量文本进行训练的。一些作者使用了大型的经过预训练的语言模型，例如BERT [Devlin等，2019]，因此必须相应地调整其方法。因此，许多工作着重于对此类语言模型进行更好的微调，而不是发明新的体系结构。

Trinh和Le [2018]率先使用了预训练的语言模型。与Opitz和Frank [2018]相似，通过用两个候选单词中的每个替换代词从每个示例中创建两个句子。一种语言模型（被实现为LSTM并在大量文本集上进行了预训练）用于为它们分配概率。在PDP（准确度为70％）和WSC273数据集（准确度为63.74％）上评估了获得的14种语言模型的整体。 Trichelair等。 [2018]已经表明，在交换候选人的情况下，这种合奏是高度不一致的，并且主要在WSC273的关联子集上工作良好。 Radford等。 [2019]使用相同的方法评估他们的GPT-2语言模型，并在WSC273上达到70.7％的准确性。 Melo等。 [2020]在他们的葡萄牙语版本的Winograd Schema Challenge上使用这种方法。他们使用了基于LSTM的语言模型，该模型经过了葡萄牙维基百科上的文本训练，但只能达到机会级的性能。

Prakash等。 [2019]通过知识搜寻扩展了这种方法。他们在网络上找到描述类似情况的句子，但可能更容易解决。他们假设代词指代同一候选词。他们使用与Trinh和Le [2018]相同的方法来计算每个代词的每个候选词的概率。描述并假设了句子中的代词和Winograd模式中的代词是指同一实体的假设，并采用了概率软逻辑[Kimmig et al。，2012]。也就是说，所有代词都以最可能的方式被解析为相同的候选项。通过将语言模型和知识搜寻相结合而获得的最佳模型，在WSC273上的精度为71.06％，在WSC285上的精度为70.17％。

Klein和Nabi [2019]分析了预先训练的基于BERT的语言模型的内部注意力层次[Devlin et al。，2019]，以找到最佳的参考对象。他们定义了一个最大注意力得分，该得分用于计算模型在所有层次和注意力头上对每个候选者的关注程度。在PDP（精度为68.3％）和WSC273（精度为60.3％）上对模型进行了评估。

Kocijan等。 [2019b]将Trinh和Le [2018]的分数改编成掩盖的语言模型，即BERT [Devlin等人，2019]。他们还从英语维基百科中引入了无监督的预训练数据集MASKEDWIKI，该数据集是通过掩盖重复出现的名词（130M个示例，缩减为2.4M个）而构建的。当同时在MASKEDWIKI和DPR上进行微调时，BERT-large在WSC273上的性能达到72.5％，在WNLI上的性能达到74.7％。通过转换第2.4节中介绍的WNLI示例，这是第一个超过多数类基准的模型。

数位作者已将此方法作为GLUE基准的一部分用于WNLI [Wang等，2019b]，T5达到94.5％的最佳性能[Raffel等，2019]。对Kocijan等人的改进。 [2019b]通常来自更广泛的预培训，以及有关WNLI训练集的培训，但Kocijan等人并未使用[2019b]，因为它与WSC273重叠。

在随后的工作中，Kocijan等人[2019a]引入了一个名为WIKICREM的数据集（240万个示例），该数据集的生成方式与MASKEDWIKI相同，但仅限于掩盖个人姓名。通过在WIKICREM上进行预训练并在其他共参考数据集上进行微调，他们在DPR上的准确性达到84.8％，在WSC273上达到71.8％，在WNLI上达到74.7％，在PDP上达到86.7％。

 Ye等 [2019]引入了针对掩蔽语言模型的对齐，掩膜和选择（AMS）预训练方法。他们发现包含在ConceptNet知识库中直接连接的实体的句子[Speer和Havasi，2012]。他们掩盖了其中一个，并训练模型从其他类似候选者的候选者列表中选择模型。他们以与Kocijan等人相同的方式微调获得的模型[2019b]在WSC273和WNLI上分别达到75.5％和83.6％的准确度。

He et al. [2019]结合了Kocijan等人的掩盖令牌预测模型[2019b]和Wang等人的语义相似性模型[2019c]创建一个混合神经网络模型。组合模型在WSC285上达到75.1％的精度，在PDP上达到90.0％的精度，在WNLI上达到89.0％的精度。 WNLI结果是通过使用合奏获得的。

Ruan等人使用了BERT语言模型的另一种用法[2019]，他们利用BERT下一句预测功能。除了用候选词代替代词外，他们还将句子分为两部分，预测第二部分在语义上是否遵循第一部分。为了提高性能，阮等人。 [2019]通过更改BERT中的某些注意张量来编码句法依赖性，并在DPR上进行训练。结合了所有功能的BERT大型模型在WSC273数据集上实现了71.1％的准确性。

Sakaguchi et al. [2020]使用相同的方法评估WINOGRANDE数据集的RoBERTa基线；但是，他们不会修改任何关注图层，而是在WINOGRANDE而非DPR上进行训练。他们报告说，在WINOGRANDE上达到了79.1％的精度，在WSC273上达到了90.1％的精度，在PDP上达到了87.5％，在WNLI上达到了85.6％，在DPR上达到了93.1％。到目前为止，这是WSC273数据集获得的最高性能，显示了更多训练数据的影响。奇怪的是，他们报告了在对WINOGRANDE进行偏置训练时，在WINO-GRANDE的验证集上实现了机会级的改进。他们怀疑引入的模型在WINO-GRANDEfull上表现良好，因为它经过训练可以利用数据集中的系统性偏差。
## Conclusion
在本文中，我们回顾并比较了已经创建的Winograd模式的数据集，以及为解决这些模式而开发的许多系统。
目前，这些系统中最好的，利用深度神经网络并结合非常大的和复杂的预训练变压器模型，如BERT或RoBERTa finetuned，能够在WSC273和类似数据集上达到90%的准确率。
Levesque等人[2012]声称，由于使用双句，“涉及词序或单词或词组的其他特征的巧妙技巧将不起作用[强调添加]。”
这一预测已被证伪，至少就那篇论文产生的数据集而言是如此。
这篇论文没有预见到神经网络的威力，自然语言建模技术的快速发展导致了像BERT这样的语言模型，也没有预见到这些技术能够发现和应用语言模式的微妙和复杂性。
在Winograd模式挑战中成功的系统在小段文本中的代词消歧任务中也成功了，但它们既没有表现出执行其他自然语言理解任务的能力，也没有表现出常识。
他们没有表现出可靠地回答关于叙事文本的简单问题的能力(Marcus和Davis, 2019)，或者回答关于日常情况的简单问题的能力。
类似地，即使使用最先进的语言建模系统(如GPT-2)生成的文本也经常包含不连贯的内容[Marcus, 2020]。
常识推理和自然语言理解社区需要新的测试，比Winograd模式挑战更深入，但仍然容易管理和评估。
已经提出了几种测试方法，似乎很有希望。
Marcus[2020]讨论了叙事文本中世界模型的进程跟踪问题。
图灵测试的许多替代方案[Marcus等人，2016]同样大量利用了各种形式的常识知识。
