
# 论文目的
在此论文中，作者们提出了一种联合常识和关系推理的图像视频文本描述生成方法。该方法通过迭代学习算法实现，交替执行以下两种推理方式：(1) 常识推理，将视觉区域根据常识推理，嵌入到语义空间中从而构成语义图；(2) 关系推理，将语义图通过图神经网络编码，生成图像视频文字描述。

# 论文方法
图像视频的文字描述本质上是视觉到语言 (Vision-to-Language，即 V2L) 的问题。作为视觉理解 (Visual Understanding) 的一个研究方向，连接着视觉与语言，模型需要在理解图像视频的基础上给出描述视觉内容的自然语言语句。该任务在人机交互或者素材检索中有着重要的应用前景。比如老师要求幼儿园或者低年级的小朋友们看图说故事，具备图像视频的文字描述技术的机器可以辅助教学，与小朋友形成互动。一个视频网站的编辑运营人员试图搜索「易烊千玺跳舞」或者「在海边散步的恋人」的视频片段。  

在现实生活中，人们可以通过常识（先验知识）来联想推理一些不易识别或不可见的事物。人们通常会用「Several people waiting at a race holding umbrellas（一些打着雨伞的人在等一场比赛）」来描述图中内容，但实际上并不能从图像中直接观测到「race（比赛）」这个语义。然而可以通过观测到的「人群围在马路两旁」来推断出这是一场比赛。而这个推断的依据就是「人群围观」和「比赛」之间的关联常识。受此启发，本文作者提出利用常识引导推断图像视频中的语义关系，进而生成文字语句的描述方法。该方法联合关系与常识推理，不依赖任何物体或关系检测器，并且在训练时也不需要场景图的标注，这样的好处是：
1. 通过额外知识的引导，探索获得难以从视觉信息中学到的物体或关系；  
2. 通过端到端的学习，可以保持图像视频和文字描述的语义一致性。  

本文提出的联合推理方法，结合以下两种推理方式：
1. 常识推理，将图像视频区域根据常识推理，嵌入到语义空间中从而构成语义图；
2. 关系推理，将语义图通过图神经网络（graph convolutional network, GCN）编码，生成图像视频的文字描述。  

该方法包含三个模块：视觉映射与知识映射、常识推理、关系推理。在视觉映射与知识映射模块中，首先稠密采样图像或视频的局部区域，将这些区域根据表观特征聚类得到候选语义（即物体、属性和关系）。然后分别使用视觉映射与知识映射学习候选语义的视觉特征向量和知识向量。在常识推理模块中，根据知识图谱来挑选候选语义组成语义图。在关系推理模块中，使用图卷积网络和基于序列的语言模型将给定的语义图经过关系推理得到图像或视频的文字描述。

## 视觉映射与知识映射
视觉映射是提取图像或视频的局部特征。首先稠密采样每个图像（视频）的 2D 图像块（3D 视频块），然后使用预训练的 CNN 提取特征，最后将这些特征聚类，用聚类中心表示候选语义的视觉特征向量。  

知识映射是将图像（视频）的候选语义映射到知识空间上。首先，将每个图像（视频）的视觉特征通过多头注意力机制（multi-head attention mechanism）映射到语义概念空间（每个图像视频的真值语义是直接由其真值句子通过词类标注工具自动得到），得到语义概念；然后学习这些语义概念的知识嵌入（knowledge embedding）；最后将知识嵌入向量作为基，语义概念的概率作为权重系数，加权相加后得到图像（视频）候选语义的知识向量。

## 联合常识和关系推理
给定图像（视频）候选语义的视觉特征向量和知识向量，迭代执行常识和关系推理训练视频（图像）文字描述模型
