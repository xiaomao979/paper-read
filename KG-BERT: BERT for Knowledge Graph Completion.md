# 摘要
知识图是许多人工智能任务的重要资源，但经常遭受不完整的困扰。 在这项工作中，我们建议使用预先训练的语言模型来完成知识图。 我们将知识图中的三元组视为文本序列，并提出了一个名为“知识图双向变压器编码器表示形式”的新颖框架（KG-BERT），以对这些三元组进行建模。 我们的方法以三元组的实体和关系描述作为输入，并使用KG-BERT语言模型计算三元组的评分函数。 在多个基准知识图上的实验结果表明，我们的方法可以在三重分类，链接预测和关系预测任务中达到最先进的性能。

# 论文目的
利用bert模型补全知识图谱

# 论文方法
 具体来说，我们首先将实体、关系和三元组作为文本序列，并将知识图完成转化为序列分类问题。 然后，我们在这些seq上微调BERT模型 用于预测三重关系或关系的合理性。  
 我们将实体和关系表示为其名称或描述，然后将名称/描述词序列作为BERT模型的输入语句进行微调。 作为原始的BERT，“句子”可以是连续文本或单词序列的任意跨度，而不是实际的语言句子。 为了模拟三元组的真实性，我们将（h，r，t）的句子打包为单个序列。   
 
 ![Image text](https://github.com/xiaomao979/paper-read/raw/master/picture/KG-BERT BERT for Knowledge Graph Completion.png)
