# 摘要
Winograd Schema Challenge（WSC）数据集WSC273及其推理对手WNLI是自然语言理解和常识推理的流行基准。 在本文中，我们表明，当在相似的代词消除歧义问题数据集（表示为WSCR）上进行微调时，WSC273上三种语言模型的性能将持续稳定地提高。 我们还生成了一个大型的无监督WSClike数据集。 通过在引入的WSCR数据集和WSCR数据集上微调BERT语言模型，我们在WSC273和WNLI上实现了72.5％和74.7％的总体准确度，从而将先前的最新解决方案提高了8.8％和9.6％.此外，我们的微调模型在Trichelair等人介绍的WSC273的“复杂”子集上也始终更加准确。

# 论文方法
