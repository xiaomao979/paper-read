# 摘要
经过大量原始文本数据训练的上下文表示法已经对NLP任务（包括问题回答和阅读理解）进行了显着改进。已经有工作表明语法，语义和词义知识包含在此类表示中，这解释了它们为何有益于此类任务。但是，有关情境化表示中所包含的常识知识的研究工作相对较少，这对于人类对问题的回答和阅读理解至关重要。我们通过在七个具有挑战性的基准测试中测试了GPT，BERT，XLNet和RoBERTa的常识能力，发现语言建模及其变体是提升模型常识能力的有效目标，而双向上下文和更大的训练集则是加分项。我们还发现，当前的模型无法很好地完成任务，需要更多必要的推理步骤。最后，我们通过制作双重测试用例来测试模型的健壮性，这些用例相互关联，这样一个样本的正确预测应该导致另一个样本的正确预测。有趣的是，这些模型在这些测试用例上显示出混乱，这表明它们在表面而不是深层学习常识。我们发布了一个测试集，公开称为CAT，以供将来研究。
# 论文目的
用五种[GPT (Radford and Sutskever 2018), GPT2 (Radford et al.2019), BERT (Devlin et al. 2019), XLNet (Yang et al. 2019) and RoBERTa (Liu et al. 2019b)]最先进的模型来评估七种[ Conjunction Acceptability, Sense Making (Wang et al.2019), Winograd Schema Challenge (Levesque, Davis, and Morgenstern 2012), SWAG (Zellers et al. 2018), HellaSwag (Zellers et al. 2019), Sense Making with Reasoning (Wang et al. 2019), and Argument Reasoning Comprehension (Habernal et al. 2018).]常识基准.

# 七种常识基准
## Sense Making (SM)

# 论文方法
1. 数据集构建通过正负样本采样
2. 为了检测模型的鲁棒性，作者将测试集中的个别单词进行编辑（删除/替换/增加），得到与原测试集相近的错误的测试集

# 论文结论
从结果来看，我们有四个明显的观察结果。   
1. 预训练的模型始终比随机基准提供更好的性能，这表明语言模型的预训练对于学习常识知识很有用。 
2. 与基于双向上下文（例如GPT和GPT2）的模型相比，基于BERT，XLNet和RoBERTa等双向上下文的模型在学习常识方面更强大。 
3. 可以从较大的训练集中学习更多常识知识，这与直觉很吻合。 
4. 模型具有一定程度的常识推理能力。 但是，随着所需推理步骤数量的增加，模型性能下降，这表明常识仍然是一个巨大的挑战，而预训练的上下文化语言模型（LM）不能完全解决常识
5. 从理论上讲，配备相关常识的模型应在一对双重测试用例上给出一致的预测。 但是，我们发现没有一个模型能够达到这样的一致性。 取而代之的是，模型被修改所迷惑，尽管它们可能具有不同的标签，但它们往往会在一对双重样本上给出相同的预测。 这进一步表明，预训练模型中包含的常识可能会停留在表面层面，而无需深入的语义理解。 我们在GitHub上公开发布了名为常识能力测试（CAT）的数据集和测试脚本


